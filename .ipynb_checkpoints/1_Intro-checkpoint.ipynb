{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to TensorFlow\n",
    "\n",
    "Check TensorFlow installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Values are encapsulated in tensors\n",
    "\n",
    "* A session is an environment for running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    "# C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feeding non-constant data use `tf.placeholder()` and `feed_dict`.\n",
    "\n",
    "`tf.placeholder()` returns a tensor that gets its value from data passed to the `tf.Session.run` function, allowing to set the input right before the session runs.\n",
    "\n",
    "`feed_dict` is a parameter of `tf.Session.run` to set the _placeholder_ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Math\n",
    "\n",
    "a = tf.add(5, 2) # 7\n",
    "s = tf.subtract(10, 4) # 6\n",
    "m = tf.multiply(2, 5)  # 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(a)\n",
    "    print sess.run(s)\n",
    "    print sess.run(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "#z = tf.subtract(tf.divide(x,y), 1) \n",
    "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classifier\n",
    "\n",
    "Linear matrix multiplication to generate predictions.\n",
    "\n",
    "$$\n",
    "y = xW + b\n",
    "$$\n",
    "\n",
    "Output are probabilities from softmax function.\n",
    "\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified.\n",
    "\n",
    "The `tf.Variable` class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. \n",
    "\n",
    "`tf.global_variables_initializer()` function initializes the state of all variable tensors.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it, it also prevents any one weight from overwhelming other weights. Use the `tf.truncated_normal()` function to generate random numbers from a normal distribution.\n",
    "\n",
    "`weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))`\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias.\n",
    "\n",
    "`bias = tf.Variable(tf.zeros(n_labels))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "```\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w),b)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "The output of the Softmax function is equivalent to a categorical probability. In Tensorflow use:\n",
    "\n",
    "[`tf.nn.softmax()`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)\n",
    "\n",
    "### Quiz:\n",
    "\n",
    "```\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding (using scikit-learn)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossentropy Loss Function\n",
    "\n",
    "Creating Cross Entropy function using `tf.reduce_sum()` - sums numbers in array-, and `tf.log()` - natural log -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nALTERNATIVE:\\n\\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\\n\\nwith tf.Session() as sess:\\n    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#softmax_data = [0.7, 0.2, 0.1]\n",
    "#one_hot_data = [1.0, 0.0, 0.0]\n",
    "softmax_data = [0.27, 0.11, 0.33, 0.10, 0.19]\n",
    "one_hot_data = [0, 0, 0, 1, 0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(tf.negative(tf.reduce_sum(tf.multiply(tf.log(softmax), one_hot))), \n",
    "                   feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
    "\n",
    "\n",
    "'''\n",
    "ALTERNATIVE:\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n",
    "'''\n",
    "\n",
    "# 0.356675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize the loss function\n",
    "\n",
    "Minimize the average cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Gradient Descent doesn't scale well, so in practice SGD is used instead.\n",
    "\n",
    "Use random batches of the data, instead of all the data, to determine the direction in the solution space. Less expensive computations but need much more of them. At the end this works better.\n",
    "\n",
    "* Needs normalized inputs (mean = 0 and equal -small- variance)\n",
    "\n",
    "* Initialize with random weights, also normalized with small variance\n",
    "\n",
    "* Keep running average of the gradients and use it to rectify the direction that specifies the current batch. This is also called 'momentum'.\n",
    "\n",
    "* Use of 'learning rate decay'. Reduce the step size as training progresses.\n",
    "\n",
    "Always try to lower learning rate first when tuning SGD. ADAGRAD is one of the models that reduces hyperparameter tuning complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatching\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# First time download of data\n",
    "# from: https://www.tensorflow.org/get_started/mnist/beginners\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172480000\n",
      "2200000\n",
      "31360\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory size of train_features, train_labels, weights, and bias in bytes. \n",
    "# (just calculate the memory required for the stored data)\n",
    "\n",
    "print(train_features.nbytes)\n",
    "print(train_labels.nbytes)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(weights).nbytes)\n",
    "    print(sess.run(bias).nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TensorFlow minibatching\n",
    "\n",
    "Create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7\\*128 + 1\\*104 = 1000)\n",
    "\n",
    "```\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "\n",
    "The `None` dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.This setup allows to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many batches are there?\n",
      "391\n",
      "What is the last batch size?\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "feature_dim = [50000, 400]\n",
    "labels_dim = [50000,10]\n",
    "batch_size = 128\n",
    "\n",
    "def total_batches(feature_dim, batch_size):\n",
    "    #ref: https://stackoverflow.com/questions/2356501/how-do-you-round-up-a-number-in-python\n",
    "    return int(feature_dim[0]/batch_size) + (feature_dim[0]%batch_size > 0)\n",
    "\n",
    "print \"How many batches are there?\"\n",
    "print total_batches(feature_dim, batch_size)\n",
    "\n",
    "def last_batch_size(feature_dim, batch_size):\n",
    "    return feature_dim[0] - (total_batches(feature_dim, batch_size) - 1) * batch_size\n",
    "\n",
    "print \"What is the last batch size?\"\n",
    "print last_batch_size(feature_dim, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'], ['F21', 'F22', 'F23', 'F24'], ['F31', 'F32', 'F33', 'F34']], [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]], [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "# Mini-batching implementation\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    batched_data = []\n",
    "    total_samples = len(features)\n",
    "    for init_index in range(0, total_samples, batch_size):\n",
    "        cut_index = init_index + batch_size\n",
    "        current_batch = [features[init_index:cut_index], labels[init_index:cut_index]]\n",
    "        batched_data.append(current_batch)\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "\n",
    "print example_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0930999964476\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in one cell:\n",
    "\n",
    "```\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    batched_data = []\n",
    "    total_samples = len(features)\n",
    "    for init_index in range(0, total_samples, batch_size):\n",
    "        cut_index = init_index + batch_size\n",
    "        current_batch = [features[init_index:cut_index], labels[init_index:cut_index]]\n",
    "        batched_data.append(current_batch)\n",
    "    return batched_data\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    - Cost: 11.8     Valid Accuracy: 0.076\n",
      "Epoch: 1    - Cost: 10.6     Valid Accuracy: 0.0902\n",
      "Epoch: 2    - Cost: 9.55     Valid Accuracy: 0.103\n",
      "Epoch: 3    - Cost: 8.73     Valid Accuracy: 0.116\n",
      "Epoch: 4    - Cost: 8.05     Valid Accuracy: 0.134\n",
      "Epoch: 5    - Cost: 7.48     Valid Accuracy: 0.15 \n",
      "Epoch: 6    - Cost: 6.99     Valid Accuracy: 0.166\n",
      "Epoch: 7    - Cost: 6.57     Valid Accuracy: 0.186\n",
      "Epoch: 8    - Cost: 6.21     Valid Accuracy: 0.2  \n",
      "Epoch: 9    - Cost: 5.88     Valid Accuracy: 0.219\n",
      "Epoch: 10   - Cost: 5.6      Valid Accuracy: 0.237\n",
      "Epoch: 11   - Cost: 5.36     Valid Accuracy: 0.253\n",
      "Epoch: 12   - Cost: 5.16     Valid Accuracy: 0.267\n",
      "Epoch: 13   - Cost: 4.97     Valid Accuracy: 0.28 \n",
      "Epoch: 14   - Cost: 4.81     Valid Accuracy: 0.298\n",
      "Epoch: 15   - Cost: 4.67     Valid Accuracy: 0.318\n",
      "Epoch: 16   - Cost: 4.54     Valid Accuracy: 0.333\n",
      "Epoch: 17   - Cost: 4.42     Valid Accuracy: 0.35 \n",
      "Epoch: 18   - Cost: 4.31     Valid Accuracy: 0.363\n",
      "Epoch: 19   - Cost: 4.21     Valid Accuracy: 0.377\n",
      "Epoch: 20   - Cost: 4.12     Valid Accuracy: 0.39 \n",
      "Epoch: 21   - Cost: 4.03     Valid Accuracy: 0.403\n",
      "Epoch: 22   - Cost: 3.94     Valid Accuracy: 0.415\n",
      "Epoch: 23   - Cost: 3.86     Valid Accuracy: 0.426\n",
      "Epoch: 24   - Cost: 3.79     Valid Accuracy: 0.438\n",
      "Epoch: 25   - Cost: 3.71     Valid Accuracy: 0.448\n",
      "Epoch: 26   - Cost: 3.64     Valid Accuracy: 0.456\n",
      "Epoch: 27   - Cost: 3.58     Valid Accuracy: 0.466\n",
      "Epoch: 28   - Cost: 3.51     Valid Accuracy: 0.475\n",
      "Epoch: 29   - Cost: 3.45     Valid Accuracy: 0.485\n",
      "Epoch: 30   - Cost: 3.4      Valid Accuracy: 0.492\n",
      "Epoch: 31   - Cost: 3.34     Valid Accuracy: 0.5  \n",
      "Epoch: 32   - Cost: 3.29     Valid Accuracy: 0.508\n",
      "Epoch: 33   - Cost: 3.24     Valid Accuracy: 0.513\n",
      "Epoch: 34   - Cost: 3.19     Valid Accuracy: 0.519\n",
      "Epoch: 35   - Cost: 3.14     Valid Accuracy: 0.526\n",
      "Epoch: 36   - Cost: 3.1      Valid Accuracy: 0.532\n",
      "Epoch: 37   - Cost: 3.05     Valid Accuracy: 0.537\n",
      "Epoch: 38   - Cost: 3.01     Valid Accuracy: 0.543\n",
      "Epoch: 39   - Cost: 2.97     Valid Accuracy: 0.548\n",
      "Epoch: 40   - Cost: 2.93     Valid Accuracy: 0.552\n",
      "Epoch: 41   - Cost: 2.89     Valid Accuracy: 0.556\n",
      "Epoch: 42   - Cost: 2.86     Valid Accuracy: 0.563\n",
      "Epoch: 43   - Cost: 2.82     Valid Accuracy: 0.567\n",
      "Epoch: 44   - Cost: 2.79     Valid Accuracy: 0.573\n",
      "Epoch: 45   - Cost: 2.76     Valid Accuracy: 0.577\n",
      "Epoch: 46   - Cost: 2.72     Valid Accuracy: 0.581\n",
      "Epoch: 47   - Cost: 2.69     Valid Accuracy: 0.585\n",
      "Epoch: 48   - Cost: 2.66     Valid Accuracy: 0.59 \n",
      "Epoch: 49   - Cost: 2.63     Valid Accuracy: 0.594\n",
      "Epoch: 50   - Cost: 2.6      Valid Accuracy: 0.598\n",
      "Epoch: 51   - Cost: 2.58     Valid Accuracy: 0.603\n",
      "Epoch: 52   - Cost: 2.55     Valid Accuracy: 0.607\n",
      "Epoch: 53   - Cost: 2.52     Valid Accuracy: 0.611\n",
      "Epoch: 54   - Cost: 2.5      Valid Accuracy: 0.615\n",
      "Epoch: 55   - Cost: 2.47     Valid Accuracy: 0.617\n",
      "Epoch: 56   - Cost: 2.45     Valid Accuracy: 0.62 \n",
      "Epoch: 57   - Cost: 2.42     Valid Accuracy: 0.625\n",
      "Epoch: 58   - Cost: 2.4      Valid Accuracy: 0.628\n",
      "Epoch: 59   - Cost: 2.38     Valid Accuracy: 0.632\n",
      "Epoch: 60   - Cost: 2.36     Valid Accuracy: 0.636\n",
      "Epoch: 61   - Cost: 2.33     Valid Accuracy: 0.639\n",
      "Epoch: 62   - Cost: 2.31     Valid Accuracy: 0.641\n",
      "Epoch: 63   - Cost: 2.29     Valid Accuracy: 0.644\n",
      "Epoch: 64   - Cost: 2.27     Valid Accuracy: 0.648\n",
      "Epoch: 65   - Cost: 2.25     Valid Accuracy: 0.65 \n",
      "Epoch: 66   - Cost: 2.23     Valid Accuracy: 0.653\n",
      "Epoch: 67   - Cost: 2.21     Valid Accuracy: 0.654\n",
      "Epoch: 68   - Cost: 2.19     Valid Accuracy: 0.656\n",
      "Epoch: 69   - Cost: 2.18     Valid Accuracy: 0.66 \n",
      "Epoch: 70   - Cost: 2.16     Valid Accuracy: 0.662\n",
      "Epoch: 71   - Cost: 2.14     Valid Accuracy: 0.664\n",
      "Epoch: 72   - Cost: 2.12     Valid Accuracy: 0.667\n",
      "Epoch: 73   - Cost: 2.11     Valid Accuracy: 0.669\n",
      "Epoch: 74   - Cost: 2.09     Valid Accuracy: 0.671\n",
      "Epoch: 75   - Cost: 2.07     Valid Accuracy: 0.674\n",
      "Epoch: 76   - Cost: 2.06     Valid Accuracy: 0.677\n",
      "Epoch: 77   - Cost: 2.04     Valid Accuracy: 0.678\n",
      "Epoch: 78   - Cost: 2.03     Valid Accuracy: 0.679\n",
      "Epoch: 79   - Cost: 2.01     Valid Accuracy: 0.681\n",
      "Test Accuracy: 0.683499991894\n"
     ]
    }
   ],
   "source": [
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "\n",
    "#n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "#n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "#mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "#features = tf.placeholder(tf.float32, [None, n_input])\n",
    "#labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "#weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "#bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "#logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 80\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
